PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): GPT2LMHeadModel(
      (transformer): GPT2Model(
        (wte): Embedding(50257, 768)
        (wpe): Embedding(1024, 768)
        (drop): Dropout(p=0.1, inplace=False)
        (h): ModuleList(
          (0-11): 12 x GPT2Block(
            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2FlashAttention2(
              (c_attn): lora.Linear(
                (base_layer): Conv1D(nf=2304, nx=768)
                (lora_dropout): ModuleDict(
                  (L_0): Dropout(p=0.1, inplace=False)
                  (L_1): Dropout(p=0.1, inplace=False)
                  (L_2): Dropout(p=0.1, inplace=False)
                  (L_3): Dropout(p=0.1, inplace=False)
                  (L_4): Dropout(p=0.1, inplace=False)
                  (L_5): Dropout(p=0.1, inplace=False)
                  (L_6): Dropout(p=0.1, inplace=False)
                  (L_7): Dropout(p=0.1, inplace=False)
                  (L_8): Dropout(p=0.1, inplace=False)
                  (L_9): Dropout(p=0.1, inplace=False)
                  (L_10): Dropout(p=0.1, inplace=False)
                  (L_11): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (L_0): Linear(in_features=768, out_features=5, bias=False)
                  (L_1): Linear(in_features=768, out_features=5, bias=False)
                  (L_2): Linear(in_features=768, out_features=5, bias=False)
                  (L_3): Linear(in_features=768, out_features=5, bias=False)
                  (L_4): Linear(in_features=768, out_features=5, bias=False)
                  (L_5): Linear(in_features=768, out_features=5, bias=False)
                  (L_6): Linear(in_features=768, out_features=5, bias=False)
                  (L_7): Linear(in_features=768, out_features=5, bias=False)
                  (L_8): Linear(in_features=768, out_features=5, bias=False)
                  (L_9): Linear(in_features=768, out_features=5, bias=False)
                  (L_10): Linear(in_features=768, out_features=5, bias=False)
                  (L_11): Linear(in_features=768, out_features=5, bias=False)
                )
                (lora_B): ModuleDict(
                  (L_0): Linear(in_features=5, out_features=2304, bias=False)
                  (L_1): Linear(in_features=5, out_features=2304, bias=False)
                  (L_2): Linear(in_features=5, out_features=2304, bias=False)
                  (L_3): Linear(in_features=5, out_features=2304, bias=False)
                  (L_4): Linear(in_features=5, out_features=2304, bias=False)
                  (L_5): Linear(in_features=5, out_features=2304, bias=False)
                  (L_6): Linear(in_features=5, out_features=2304, bias=False)
                  (L_7): Linear(in_features=5, out_features=2304, bias=False)
                  (L_8): Linear(in_features=5, out_features=2304, bias=False)
                  (L_9): Linear(in_features=5, out_features=2304, bias=False)
                  (L_10): Linear(in_features=5, out_features=2304, bias=False)
                  (L_11): Linear(in_features=5, out_features=2304, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (c_proj): lora.Linear(
                (base_layer): Conv1D(nf=768, nx=768)
                (lora_dropout): ModuleDict(
                  (L_0): Dropout(p=0.1, inplace=False)
                  (L_1): Dropout(p=0.1, inplace=False)
                  (L_2): Dropout(p=0.1, inplace=False)
                  (L_3): Dropout(p=0.1, inplace=False)
                  (L_4): Dropout(p=0.1, inplace=False)
                  (L_5): Dropout(p=0.1, inplace=False)
                  (L_6): Dropout(p=0.1, inplace=False)
                  (L_7): Dropout(p=0.1, inplace=False)
                  (L_8): Dropout(p=0.1, inplace=False)
                  (L_9): Dropout(p=0.1, inplace=False)
                  (L_10): Dropout(p=0.1, inplace=False)
                  (L_11): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (L_0): Linear(in_features=768, out_features=5, bias=False)
                  (L_1): Linear(in_features=768, out_features=5, bias=False)
                  (L_2): Linear(in_features=768, out_features=5, bias=False)
                  (L_3): Linear(in_features=768, out_features=5, bias=False)
                  (L_4): Linear(in_features=768, out_features=5, bias=False)
                  (L_5): Linear(in_features=768, out_features=5, bias=False)
                  (L_6): Linear(in_features=768, out_features=5, bias=False)
                  (L_7): Linear(in_features=768, out_features=5, bias=False)
                  (L_8): Linear(in_features=768, out_features=5, bias=False)
                  (L_9): Linear(in_features=768, out_features=5, bias=False)
                  (L_10): Linear(in_features=768, out_features=5, bias=False)
                  (L_11): Linear(in_features=768, out_features=5, bias=False)
                )
                (lora_B): ModuleDict(
                  (L_0): Linear(in_features=5, out_features=768, bias=False)
                  (L_1): Linear(in_features=5, out_features=768, bias=False)
                  (L_2): Linear(in_features=5, out_features=768, bias=False)
                  (L_3): Linear(in_features=5, out_features=768, bias=False)
                  (L_4): Linear(in_features=5, out_features=768, bias=False)
                  (L_5): Linear(in_features=5, out_features=768, bias=False)
                  (L_6): Linear(in_features=5, out_features=768, bias=False)
                  (L_7): Linear(in_features=5, out_features=768, bias=False)
                  (L_8): Linear(in_features=5, out_features=768, bias=False)
                  (L_9): Linear(in_features=5, out_features=768, bias=False)
                  (L_10): Linear(in_features=5, out_features=768, bias=False)
                  (L_11): Linear(in_features=5, out_features=768, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): Conv1D(nf=3072, nx=768)
              (c_proj): lora.Linear(
                (base_layer): Conv1D(nf=768, nx=3072)
                (lora_dropout): ModuleDict(
                  (L_0): Dropout(p=0.1, inplace=False)
                  (L_1): Dropout(p=0.1, inplace=False)
                  (L_2): Dropout(p=0.1, inplace=False)
                  (L_3): Dropout(p=0.1, inplace=False)
                  (L_4): Dropout(p=0.1, inplace=False)
                  (L_5): Dropout(p=0.1, inplace=False)
                  (L_6): Dropout(p=0.1, inplace=False)
                  (L_7): Dropout(p=0.1, inplace=False)
                  (L_8): Dropout(p=0.1, inplace=False)
                  (L_9): Dropout(p=0.1, inplace=False)
                  (L_10): Dropout(p=0.1, inplace=False)
                  (L_11): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (L_0): Linear(in_features=3072, out_features=5, bias=False)
                  (L_1): Linear(in_features=3072, out_features=5, bias=False)
                  (L_2): Linear(in_features=3072, out_features=5, bias=False)
                  (L_3): Linear(in_features=3072, out_features=5, bias=False)
                  (L_4): Linear(in_features=3072, out_features=5, bias=False)
                  (L_5): Linear(in_features=3072, out_features=5, bias=False)
                  (L_6): Linear(in_features=3072, out_features=5, bias=False)
                  (L_7): Linear(in_features=3072, out_features=5, bias=False)
                  (L_8): Linear(in_features=3072, out_features=5, bias=False)
                  (L_9): Linear(in_features=3072, out_features=5, bias=False)
                  (L_10): Linear(in_features=3072, out_features=5, bias=False)
                  (L_11): Linear(in_features=3072, out_features=5, bias=False)
                )
                (lora_B): ModuleDict(
                  (L_0): Linear(in_features=5, out_features=768, bias=False)
                  (L_1): Linear(in_features=5, out_features=768, bias=False)
                  (L_2): Linear(in_features=5, out_features=768, bias=False)
                  (L_3): Linear(in_features=5, out_features=768, bias=False)
                  (L_4): Linear(in_features=5, out_features=768, bias=False)
                  (L_5): Linear(in_features=5, out_features=768, bias=False)
                  (L_6): Linear(in_features=5, out_features=768, bias=False)
                  (L_7): Linear(in_features=5, out_features=768, bias=False)
                  (L_8): Linear(in_features=5, out_features=768, bias=False)
                  (L_9): Linear(in_features=5, out_features=768, bias=False)
                  (L_10): Linear(in_features=5, out_features=768, bias=False)
                  (L_11): Linear(in_features=5, out_features=768, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (lm_head): Linear(in_features=768, out_features=50257, bias=False)
    )
  )
)